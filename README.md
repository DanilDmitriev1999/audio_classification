# :octocat: audio_classification :octocat:
Задача заключалась в классификации аудиофайлов на 2 класса.

Основной ноутбук: `inference.ipynb`

# Данные
Данные хранятся в папке `vox-test-audio`. Общее кол-во примеров 150 и два класса - **oksana** и **omazh**. Для теста из каждого класса взял по 5 примеров. 

## Подготовка данных
Данные были в формате `waw` с помощью библиотеки `librosa` привел их к MFCC формату. Обучающие данные аугментировал: (изменял скорость, добавлял шум и смещал)

# Метрики
Вычислял *f-score*, так как данные несбалансированны.

# Подход
Использовал модель на CNN. Пробовал добавлять GRU и обучать только с GRU, но наиболее стабильное обучение проходило с CNN. Так же использовал Focal Loss, но значимых изменений он не дал.

# Файлы
Я буду приводить описание файлов, которые находятся в **src**.

`DataModule.py` - класс `AudioPreProcessing` - полный пайплайн преодобработки аудио (чтение, аугментация), класс `AudioDataset` - Dataset для DataLoader

`FocalLoss.py` - Лосс функция для обучения

`model.py` - Основная модель

`trainer.py` - Обучающий класс, для него использовал `pytorch-lightning`
